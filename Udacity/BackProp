{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BackProp","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"-TLWJZRC2cEG","colab_type":"code","colab":{}},"cell_type":"code","source":["#\n","#   In the following exercises we will complete several functions for \n","#   a simple implementation of neural networks based on code by Roland \n","#   Szabo. \n","#\n","#   In this exercise, we will begin by writing a function, deltas(), \n","#   which will compute and store delta factors for each node in a \n","#   layer, given the deltas for the previous layer.\n","#\n","#   Recall that the delta value associated to an output node is the \n","#   activation_derivative \n","#   of the node's last_input multiplied by the difference of its expected output minus \n","#   its actual output\n","#\n","#   The delta value associated to a hidden node is the activation_derivative of the \n","#   node's last_input times the sum over the next layer of the products of each nodes\n","#   delta value times weight from the current node\n","#\n","#   NOTE: the following exercises creating classes for functioning\n","#   neural networks are HARD, and are not efficient implementations.\n","#   Consider them an extra challenge, not a requirement!\n","\n","\n","\n","import numpy as np\n","\n","\n","def logistic(x):\n","    return 1/(1 + np.exp(-x))\n","\n","def logistic_derivative(x):\n","    return logistic(x)*(1-logistic(x))\n","\n","class Sigmoid:\n","\n","    # default weights for an input node, usually changed when initialized\n","    weights = [1]\n","    # keeps track of previous input strengths for backpropagation\n","    last_input = 0\n","    # space to keep track of deltas for backpropagation\n","    delta = 0\n","\n","    def activate(self,values):\n","        '''Takes in @param values, @param weights lists of numbers\n","        and @param threshold a single number.\n","        @return the output of a threshold perceptron with\n","        given weights and threshold, given values as inputs.\n","        '''\n","\n","        #First calculate the strength with which the perceptron fires\n","        strength = self.strength(values)\n","        self.last_input = strength\n","\n","        result = logistic(strength)\n","\n","        return result\n","\n","    def strength(self,values):\n","        # Formats inputs to easily compute a dot product\n","        local = np.atleast_2d(self.weights)\n","        values = np.transpose(np.atleast_2d(values))\n","        strength = np.dot(local,values)\n","        return float(strength)\n","\n","    def __init__(self,weights=None):\n","        if type(weights) in [type([]), type(np.array([]))]:\n","            self.weights = weights\n","\n","\n","\n","\n","class NeuralNetwork:\n","\n","    def __init__(self, layers):\n","        \"\"\"\n","        :param layers: A list containing the number of units in each layer. Should be at least two values\n","        \"\"\"\n","\n","        self.nodes = [[]]\n","        #input nodes\n","        for j in range(0,layers[0]):\n","            self.nodes[0].append(Sigmoid())\n","        #randomly initialize weights\n","        for i in range(1, len(layers)-1):\n","            self.nodes.append([])\n","            for j in range(0,layers[i]+1):\n","                self.nodes[-1].append(Sigmoid((2*np.random.random(layers[i - 1]+1)-1)*.25))\n","        self.nodes.append([])\n","        for j in range(0,layers[i+1]):\n","            self.nodes[-1].append(Sigmoid((2*np.random.random(layers[i]+1)-1)*.25))\n","        \n","\n","\n","    def predict(self, x):\n","        \"\"\"\n","        :param x: a 1D ndarray of input values\n","        :return: a 1D ndarray of values of output nodes\n","        \"\"\"\n","        x = np.array(x)\n","        a=np.ones(x.shape[0]+1)\n","        a[0:-1]=x\n","        for l in range(1, len(self.nodes)):\n","            a = [node.activate(a) for node in self.nodes[l]]\n","        return a\n","\n","    def deltas(self,expected,outputs,layer):\n","        '''\n","        :param expected: an array of expected outputs (in the case of an output layer) or deltas from the previous layer (in the case of an input layer)\n","        :param ouptuts: an array of actual outputs from the layer\n","        :param layer: which layer of the network to update.\n","        sets the delta values for the units in the layer\n","        :returns: a list of the delta values for use in the next previous layer\n","        '''\n","\n","        #YOUR CODE HERE\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3UCVjL_3F5tR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"74fce72c-e60f-4f07-f96c-d729c9b6decb","executionInfo":{"status":"ok","timestamp":1547381366476,"user_tz":-180,"elapsed":58744,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["n=NeuralNetwork([3,2,1])\n","n.predict([5,5,5])"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.5223919450562683]"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"id":"FHBpkASoH2Ef","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2dedb6e0-a45e-4405-f6c2-5de13da690dc","executionInfo":{"status":"ok","timestamp":1547381600200,"user_tz":-180,"elapsed":1080,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["print n.nodes[0]"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[<__main__.Sigmoid instance at 0x7faa7dab17a0>, <__main__.Sigmoid instance at 0x7faa7dab1680>, <__main__.Sigmoid instance at 0x7faa7dab1638>]\n"],"name":"stdout"}]},{"metadata":{"id":"qNMaX5JME1L3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a841a202-3af3-4ed8-93fd-7e45e38cb671","executionInfo":{"status":"ok","timestamp":1547314227575,"user_tz":-180,"elapsed":12333,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["import numpy as np\n","x=[5,5,5,5]\n","x = np.array(x)\n","a=np.ones(x.shape[0]+1)\n","a[0:-1]=x\n","print a"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[5. 5. 5. 5. 1.]\n"],"name":"stdout"}]},{"metadata":{"id":"T8hAWiFfHCk3","colab_type":"code","colab":{}},"cell_type":"code","source":["In the following exercises we will complete several functions for a simple \n","#   implementation of neural networks based on code by Roland Szabo.\n","#\n","#   In this exercise, we will begin writing a function, fit(), which will train our \n","#   network on data that we provide.\n","#\n","#   Special thanks to Roland Szabo for the use of his code as a basis for this and \n","#   preceding exercises. His original code can be found at \n","#   http://rolisz.ro/2013/04/18/neural-networks-in-python/\n","#\n","#   NOTE: this and preceding exercises creating classes for functioning\n","#   neural networks are HARD, and are not efficient implementations.\n","#   Consider them an extra challenge, not a requirement!\n","\n","\n","\n","import numpy as np\n","\n","#choose a seed for testing in the exercise\n","np.random.seed(1)\n","\n","def logistic(x):\n","    return 1/(1 + np.exp(-x))\n","\n","def logistic_derivative(x):\n","    return logistic(x)*(1-logistic(x))\n","\n","class Sigmoid:\n","\n","    # default weights for an input node, usually changed when initialized\n","    weights = [1]\n","    # keeps track of previous input strengths for backpropagation\n","    last_input = 0\n","    # space to keep track of deltas for backpropagation\n","    delta = 0\n","\n","    def activate(self,values):\n","        '''Takes in @param values, @param weights lists of numbers\n","        and @param threshold a single number.\n","        @return the output of a threshold perceptron with\n","        given weights and threshold, given values as inputs.\n","        '''\n","\n","        #First calculate the strength with which the perceptron fires\n","        strength = self.strength(values)\n","        self.last_input = strength\n","\n","        result = logistic(strength)\n","\n","        return result\n","\n","    def strength(self,values):\n","        # Formats inputs to easily compute a dot product\n","        local = np.atleast_2d(self.weights)\n","        values = np.transpose(np.atleast_2d(values))\n","        strength = np.dot(local,values)\n","        return float(strength)\n","\n","    def __init__(self,weights):\n","        if type(weights) in [type([]),type(np.array([]))]:\n","            self.weights = weights\n","\n","\n","class NeuralNetwork:\n","\n","    def __init__(self, layers):\n","        \"\"\"\n","        :param layers:  A list containing the number of units in each layer. Should be \n","                        at least two values\n","        \"\"\"\n","        \n","        self.nodes = [[]]\n","        #input nodes\n","        for j in range(0,layers[0]):\n","            self.nodes[0].append(Sigmoid())\n","        #randomly initialize weights\n","        for i in range(1, len(layers)-1):\n","            self.nodes.append([])\n","            for j in range(0,layers[i]+1):\n","                self.nodes[-1].append(Sigmoid((2*np.random.random(layers[i - 1]+1)-1)*.25))\n","        self.nodes.append([])\n","        for j in range(0,layers[i+1]):\n","            self.nodes[-1].append(Sigmoid((2*np.random.random(layers[i]+1)-1)*.25))\n","        \n","#        for x in self.nodes:\n","#            print len(x),\"Layer\",len(x[-1].weights)\n","\n","\n","    def predict(self, x):\n","        \"\"\"\n","        :param x: a 1D ndarray of input values\n","        :return: a 1D ndarray of values of output nodes\n","        \"\"\"\n","        a=np.ones(x.shape[0]+1)\n","        a[0:-1]=x\n","        for l in range(1, len(self.nodes)):\n","            a = [node.activate(a) for node in self.nodes[l]]\n","#            print a\n","        return a\n","\n","\n","    def BackPropagation(self, X, y, learning_rate=0.2, epochs=3000):\n","        \"\"\"\n","        :param X: a 2D ndarray of many input values\n","        :param y: a 2D ndarray of corresponding desired output vectors\n","        :param learning_rate: controls the learning rate (optional)\n","        :param epochs: controls the number of training iterations (optional)\n","        \"\"\"\n","\n","        #YOUR CODE HERE\n","\n","        #In each epoch, we will choose and train on an example from X, y\n","\n","        #to train on each example, we will first need to evaluate the example from X\n","        #storing the signal strength at each node before the activation is applied.\n","\n","        #Then compare the outputs in y to our outputs, and scale them by the \n","        #activation_derivative(strength) at the signal strengths for each of the output\n","        #nodes.\n","        \n","        #Iterate backwards over the layers, using the deltas method below to associate a\n","        #rate of change to each node\n","\n","        #then modify each of the (non-input) node's weights by the learning rate times  \n","        #the current node's delta times the previous node's last input.\n","\n","\n","\n","    def deltas(self,y,outputs,layer):\n","        '''\n","        :param y: an array of expected outputs\n","        :param ouptuts: an array of actual outputs from the layer\n","        :param layer: which layer of the network to update. Use -1 for output layer.\n","        sets the delta values for the units in the layer\n","        :returns null:\n","        '''\n","\n","        if layer==-1:\n","            final = [y[i]-outputs[i] for i in range(0,len(y))]\n","        else:\n","            final = []\n","            for i in range(0,len(self.nodes[layer])):\n","                sum=0\n","                for j in range(0,len(self.nodes[layer+1])):\n","                    sum+= self.nodes[layer+1][j].weights[i] * self.nodes[layer+1][j].delta\n","                final.append(sum)\n","        for i in range(0,len(self.nodes[layer])):\n","            self.nodes[layer][i].delta = logistic_derivative(outputs[i])*final[i]\n","\n","\n"],"execution_count":0,"outputs":[]}]}