{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_in_tensorflow","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"54snqQ6GKTB7","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-R1lYsCKrwW","colab_type":"code","colab":{}},"source":["k_output = 64# число фильтров\n","\n","#размеры изображения\n","image_width = 10\n","image_height = 10\n","color_channel = 3\n","\n","#размеры окна конволюшна\n","filter_size_width = 5\n","filter_size_height = 5\n","\n","#input/image\n","input = tf.placeholder(tf.float32,shape=[None, image_height, image_width, color_channel])\n","\n","#weight and bias\n","weight = tf.Variable(tf.truncated_normal([filter_size_height, filter_size_width, color_channels, k_output]))\n","bias = tf.Variable(tf.zeros(k_output))\n","\n","conv_layer = tf.nn.conv2d(input, weight, strides=[1,2,2,1], padding='SAME')\n","conv_layer = tf.nn.bias_add(conv_layer, bias)\n","conv_layer = tf.nn.relu(conv_layer)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lvvmiRVMMne9","colab_type":"text"},"source":["###Объяснения stride \n","1. Tensorflow нуждается в страйде для всех измерений input, то есть - [batch, input_height, input_width, input_channels]\n","2. В обычном случае значения для stride в batch and color_channels = 1, так как мы же не хотим пропускать бачи и цветовые каналы, если же есть нужда в том, чтобы не использовать что-то из этого обычной практикой является исключение этого из датасета\n","3. tf.nn.bias_add добавляет 1-д байес к последней размерности матрицы(tf.add не работает, когдаразмерность тензоров не совпадает)"]},{"cell_type":"code","metadata":{"id":"w8Mr2ihRMiYy","colab_type":"code","colab":{}},"source":["\"\"\"\n","Setup the strides, padding and filter weight/bias such that\n","the output shape is (1, 2, 2, 3).\n","\"\"\"\n","import tensorflow as tf\n","import numpy as np\n","\n","# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\n","# (1, 4, 4, 1)\n","x = np.array([\n","    [0, 1, 0.5, 10],\n","    [2, 2.5, 1, -8],\n","    [4, 0, 5, 6],\n","    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n","X = tf.constant(x)\n","\n","\n","def conv2d(input):\n","    # Filter (weights and bias)\n","    # The shape of the filter weight is (height, width, input_depth, output_depth)\n","    # The shape of the filter bias is (output_depth,)\n","    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\n","    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n","    F_W = tf.Variable(tf.truncated_normal([input.shape[1].value, input.shape[2].value, input.shape[3].value, 3]))\n","    F_b = tf.Variable(tf.zeros(3))\n","    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n","    strides = [1, 2, 2, 1]\n","    # TODO: set the padding, either 'VALID' or 'SAME'.\n","    padding = 'SAME'\n","    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\n","    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\n","    return tf.nn.bias_add(tf.nn.conv2d(input, F_W, strides, padding), F_b)\n","\n","out = conv2d(X)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lnP-5iYRSGei","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"125b351c-094c-433d-bfbe-c98b839ed5f4","executionInfo":{"status":"ok","timestamp":1562182874824,"user_tz":-180,"elapsed":959,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["out.shape"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([Dimension(1), Dimension(2), Dimension(2), Dimension(3)])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"JS6n7_BTSdOD","colab_type":"code","colab":{}},"source":["#maxPooling layer\n","conv_layer = tf.nn.max_pool(conv_layer, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djA16cVpXhaN","colab_type":"text"},"source":["**padding='SAME'**\n","\n","$$new_d = (old_d-conv_d+2*padding)/stride +1$$"]},{"cell_type":"code","metadata":{"id":"sQm5UikVXL2J","colab_type":"code","colab":{}},"source":["\"\"\"\n","Set the values to `strides` and `ksize` such that\n","the output shape after pooling is (1, 2, 2, 1).\n","\"\"\"\n","import tensorflow as tf\n","import numpy as np\n","\n","# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)\n","# (1, 4, 4, 1)\n","x = np.array([\n","    [0, 1, 0.5, 10],\n","    [2, 2.5, 1, -8],\n","    [4, 0, 5, 6],\n","    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n","X = tf.constant(x)\n","\n","def maxpool(input):\n","    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\n","    ksize = [1, 2, 2, 1]\n","    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n","    strides = [1, 2, 2, 1]\n","    # TODO: set the padding, either 'VALID' or 'SAME'.\n","    padding = 'VALID'\n","    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool\n","    return tf.nn.max_pool(input, ksize, strides, padding)\n","    \n","out = maxpool(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-cMctfKYRnk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"7b335f6c-d98a-4e69-9ccb-14cf96d14b9d","executionInfo":{"status":"ok","timestamp":1562183291563,"user_tz":-180,"elapsed":1010,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["out.shape"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([Dimension(1), Dimension(2), Dimension(2), Dimension(1)])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"F_VBVhzMYTHy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"32a7bff0-45a3-4cc4-941a-8eb687d260af","executionInfo":{"status":"error","timestamp":1562184862683,"user_tz":-180,"elapsed":78649,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n","\n","import tensorflow as tf\n","\n","# parameters\n","learning_rate = 0.00001\n","epochs = 10\n","batch_size = 128\n","\n","# number of samples to calculate validation and accuracy\n","# decrease this if you're running out of memory\n","test_valid_size = 256\n","\n","# network Parameters\n","n_classes = 10  # MNIST total classes (0-9 digits)\n","dropout = 0.75  # dropout (probability to keep units)\n","\n","# store weights & biases\n","weights = {\n","    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n","    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n","    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n","    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n","\n","biases = {\n","    'bc1': tf.Variable(tf.random_normal([32])),\n","    'bc2': tf.Variable(tf.random_normal([64])),\n","    'bd1': tf.Variable(tf.random_normal([1024])),\n","    'out': tf.Variable(tf.random_normal([n_classes]))}\n","\n","def conv2d(x, W, b, strides=1):\n","    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n","    x = tf.nn.bias_add(x, b)\n","    return tf.nn.relu(x)\n","  \n","def maxpool2d(x, k=2):\n","    return tf.nn.max_pool(\n","        x,\n","        ksize=[1, k, k, 1],\n","        strides=[1, k, k, 1],\n","        padding='SAME')\n","  \n","def conv_net(x, weights, biases, dropout):\n","    # Layer 1 - 28*28*1 to 14*14*32\n","    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n","    conv1 = maxpool2d(conv1, k=2)\n","\n","    # Layer 2 - 14*14*32 to 7*7*64\n","    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n","    conv2 = maxpool2d(conv2, k=2)\n","\n","    # Fully connected layer - 7*7*64 to 1024\n","    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n","    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n","    fc1 = tf.nn.relu(fc1)\n","    fc1 = tf.nn.dropout(fc1, dropout)\n","\n","    # Output Layer - class prediction - 1024 to 10\n","    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n","    return out\n","\n","# tf Graph input\n","x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n","y = tf.placeholder(tf.float32, [None, n_classes])\n","keep_prob = tf.placeholder(tf.float32)\n","\n","# Model\n","logits = conv_net(x, weights, biases, keep_prob)\n","\n","# Define loss and optimizer\n","cost = tf.reduce_mean(\\\n","    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n","    .minimize(cost)\n","\n","# Accuracy\n","correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","# Initializing the variables\n","init = tf. global_variables_initializer()\n","\n","# Launch the graph\n","with tf.Session() as sess:\n","    sess.run(init)\n","\n","    for epoch in range(epochs):\n","        for batch in range(mnist.train.num_examples//batch_size):\n","            batch_x, batch_y = mnist.train.next_batch(batch_size)\n","            sess.run(optimizer, feed_dict={\n","                x: batch_x,\n","                y: batch_y,\n","                keep_prob: dropout})\n","\n","            # Calculate batch loss and accuracy\n","            loss = sess.run(cost, feed_dict={\n","                x: batch_x,\n","                y: batch_y,\n","                keep_prob: 1.})\n","            valid_acc = sess.run(accuracy, feed_dict={\n","                x: mnist.validation.images[:test_valid_size],\n","                y: mnist.validation.labels[:test_valid_size],\n","                keep_prob: 1.})\n","\n","            print('Epoch {:>2}, Batch {:>3} -'\n","                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n","                epoch + 1,\n","                batch + 1,\n","                loss,\n","                valid_acc))\n","\n","    # Calculate Test Accuracy\n","    test_acc = sess.run(accuracy, feed_dict={\n","        x: mnist.test.images[:test_valid_size],\n","        y: mnist.test.labels[:test_valid_size],\n","        keep_prob: 1.})\n","    print('Testing Accuracy: {}'.format(test_acc))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0703 20:13:07.366135 140148616296320 deprecation.py:323] From <ipython-input-17-32a033164bf4>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","W0703 20:13:07.368066 140148616296320 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please write your own downloading logic.\n","W0703 20:13:07.369182 140148616296320 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use urllib or similar directly.\n","W0703 20:13:07.475636 140148616296320 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n"],"name":"stderr"},{"output_type":"stream","text":["Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n","Extracting ./train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["W0703 20:13:07.772626 140148616296320 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","W0703 20:13:07.775330 140148616296320 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.one_hot on tensors.\n","W0703 20:13:07.870719 140148616296320 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"],"name":"stderr"},{"output_type":"stream","text":["Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n","Extracting ./train-labels-idx1-ubyte.gz\n","Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n","Extracting ./t10k-images-idx3-ubyte.gz\n","Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n","Extracting ./t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["W0703 20:13:08.234828 140148616296320 deprecation.py:506] From <ipython-input-17-32a033164bf4>:57: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0703 20:13:08.252221 140148616296320 deprecation.py:323] From <ipython-input-17-32a033164bf4>:72: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch  1, Batch   1 -Loss: 50743.1250 Validation Accuracy: 0.085938\n","Epoch  1, Batch   2 -Loss: 39198.9453 Validation Accuracy: 0.097656\n","Epoch  1, Batch   3 -Loss: 39622.5625 Validation Accuracy: 0.113281\n","Epoch  1, Batch   4 -Loss: 35053.8594 Validation Accuracy: 0.128906\n","Epoch  1, Batch   5 -Loss: 29822.9844 Validation Accuracy: 0.128906\n","Epoch  1, Batch   6 -Loss: 30460.2266 Validation Accuracy: 0.140625\n","Epoch  1, Batch   7 -Loss: 23734.9863 Validation Accuracy: 0.148438\n","Epoch  1, Batch   8 -Loss: 29064.7852 Validation Accuracy: 0.148438\n","Epoch  1, Batch   9 -Loss: 25289.2070 Validation Accuracy: 0.144531\n","Epoch  1, Batch  10 -Loss: 22446.2715 Validation Accuracy: 0.160156\n","Epoch  1, Batch  11 -Loss: 23101.0371 Validation Accuracy: 0.156250\n","Epoch  1, Batch  12 -Loss: 24427.9863 Validation Accuracy: 0.160156\n","Epoch  1, Batch  13 -Loss: 21721.0020 Validation Accuracy: 0.164062\n","Epoch  1, Batch  14 -Loss: 21611.2070 Validation Accuracy: 0.179688\n","Epoch  1, Batch  15 -Loss: 21557.5957 Validation Accuracy: 0.183594\n","Epoch  1, Batch  16 -Loss: 21202.0430 Validation Accuracy: 0.199219\n","Epoch  1, Batch  17 -Loss: 20580.1602 Validation Accuracy: 0.214844\n","Epoch  1, Batch  18 -Loss: 19720.6953 Validation Accuracy: 0.222656\n","Epoch  1, Batch  19 -Loss: 17231.4199 Validation Accuracy: 0.246094\n","Epoch  1, Batch  20 -Loss: 19846.9492 Validation Accuracy: 0.242188\n","Epoch  1, Batch  21 -Loss: 15935.2510 Validation Accuracy: 0.261719\n","Epoch  1, Batch  22 -Loss: 17908.1133 Validation Accuracy: 0.273438\n","Epoch  1, Batch  23 -Loss: 15691.1133 Validation Accuracy: 0.273438\n","Epoch  1, Batch  24 -Loss: 15555.5371 Validation Accuracy: 0.300781\n","Epoch  1, Batch  25 -Loss: 15502.9727 Validation Accuracy: 0.300781\n","Epoch  1, Batch  26 -Loss: 14764.5000 Validation Accuracy: 0.312500\n","Epoch  1, Batch  27 -Loss: 14201.0234 Validation Accuracy: 0.316406\n","Epoch  1, Batch  28 -Loss: 14088.4414 Validation Accuracy: 0.304688\n","Epoch  1, Batch  29 -Loss: 14505.6562 Validation Accuracy: 0.324219\n","Epoch  1, Batch  30 -Loss: 11390.4062 Validation Accuracy: 0.335938\n","Epoch  1, Batch  31 -Loss: 14978.5273 Validation Accuracy: 0.339844\n","Epoch  1, Batch  32 -Loss: 14524.9131 Validation Accuracy: 0.343750\n","Epoch  1, Batch  33 -Loss: 12851.3066 Validation Accuracy: 0.343750\n","Epoch  1, Batch  34 -Loss:  7966.9473 Validation Accuracy: 0.363281\n","Epoch  1, Batch  35 -Loss: 13795.9238 Validation Accuracy: 0.378906\n","Epoch  1, Batch  36 -Loss:  9830.4238 Validation Accuracy: 0.382812\n","Epoch  1, Batch  37 -Loss: 10341.2412 Validation Accuracy: 0.386719\n","Epoch  1, Batch  38 -Loss: 12934.9746 Validation Accuracy: 0.386719\n","Epoch  1, Batch  39 -Loss: 11593.7598 Validation Accuracy: 0.390625\n","Epoch  1, Batch  40 -Loss: 11687.3906 Validation Accuracy: 0.382812\n","Epoch  1, Batch  41 -Loss: 10460.6357 Validation Accuracy: 0.414062\n","Epoch  1, Batch  42 -Loss: 11124.4229 Validation Accuracy: 0.394531\n","Epoch  1, Batch  43 -Loss:  9594.8467 Validation Accuracy: 0.402344\n","Epoch  1, Batch  44 -Loss: 10739.6230 Validation Accuracy: 0.410156\n","Epoch  1, Batch  45 -Loss: 10213.5918 Validation Accuracy: 0.425781\n","Epoch  1, Batch  46 -Loss:  8688.4473 Validation Accuracy: 0.425781\n","Epoch  1, Batch  47 -Loss: 10840.6875 Validation Accuracy: 0.421875\n","Epoch  1, Batch  48 -Loss:  9200.1934 Validation Accuracy: 0.441406\n","Epoch  1, Batch  49 -Loss: 10543.5078 Validation Accuracy: 0.445312\n","Epoch  1, Batch  50 -Loss:  9894.5898 Validation Accuracy: 0.437500\n","Epoch  1, Batch  51 -Loss: 11031.4502 Validation Accuracy: 0.437500\n","Epoch  1, Batch  52 -Loss:  9511.3975 Validation Accuracy: 0.445312\n","Epoch  1, Batch  53 -Loss:  8807.0420 Validation Accuracy: 0.457031\n","Epoch  1, Batch  54 -Loss:  8279.2402 Validation Accuracy: 0.468750\n","Epoch  1, Batch  55 -Loss:  9798.7285 Validation Accuracy: 0.464844\n","Epoch  1, Batch  56 -Loss:  7995.4268 Validation Accuracy: 0.472656\n","Epoch  1, Batch  57 -Loss:  8046.1787 Validation Accuracy: 0.484375\n","Epoch  1, Batch  58 -Loss:  7948.5249 Validation Accuracy: 0.488281\n","Epoch  1, Batch  59 -Loss: 10170.3828 Validation Accuracy: 0.484375\n","Epoch  1, Batch  60 -Loss:  8544.9453 Validation Accuracy: 0.503906\n","Epoch  1, Batch  61 -Loss:  8908.6035 Validation Accuracy: 0.500000\n","Epoch  1, Batch  62 -Loss:  7190.9424 Validation Accuracy: 0.503906\n","Epoch  1, Batch  63 -Loss:  6828.3682 Validation Accuracy: 0.503906\n","Epoch  1, Batch  64 -Loss:  7676.3555 Validation Accuracy: 0.519531\n","Epoch  1, Batch  65 -Loss:  6364.8682 Validation Accuracy: 0.531250\n","Epoch  1, Batch  66 -Loss:  9091.6611 Validation Accuracy: 0.535156\n","Epoch  1, Batch  67 -Loss:  5608.3887 Validation Accuracy: 0.531250\n","Epoch  1, Batch  68 -Loss:  6504.8867 Validation Accuracy: 0.535156\n","Epoch  1, Batch  69 -Loss:  7286.9517 Validation Accuracy: 0.535156\n","Epoch  1, Batch  70 -Loss:  6928.7090 Validation Accuracy: 0.546875\n","Epoch  1, Batch  71 -Loss:  5813.0142 Validation Accuracy: 0.546875\n","Epoch  1, Batch  72 -Loss:  6897.2183 Validation Accuracy: 0.523438\n","Epoch  1, Batch  73 -Loss:  7270.2632 Validation Accuracy: 0.539062\n","Epoch  1, Batch  74 -Loss:  7479.7959 Validation Accuracy: 0.535156\n","Epoch  1, Batch  75 -Loss:  6507.7305 Validation Accuracy: 0.546875\n","Epoch  1, Batch  76 -Loss:  8076.4131 Validation Accuracy: 0.531250\n","Epoch  1, Batch  77 -Loss:  6837.2881 Validation Accuracy: 0.535156\n","Epoch  1, Batch  78 -Loss:  6611.2559 Validation Accuracy: 0.542969\n","Epoch  1, Batch  79 -Loss:  6333.7930 Validation Accuracy: 0.535156\n","Epoch  1, Batch  80 -Loss:  6535.3857 Validation Accuracy: 0.535156\n","Epoch  1, Batch  81 -Loss:  5711.7441 Validation Accuracy: 0.550781\n","Epoch  1, Batch  82 -Loss:  6280.8916 Validation Accuracy: 0.566406\n","Epoch  1, Batch  83 -Loss:  6700.8677 Validation Accuracy: 0.546875\n","Epoch  1, Batch  84 -Loss:  5189.7373 Validation Accuracy: 0.550781\n","Epoch  1, Batch  85 -Loss:  4917.7998 Validation Accuracy: 0.550781\n","Epoch  1, Batch  86 -Loss:  6902.9600 Validation Accuracy: 0.554688\n","Epoch  1, Batch  87 -Loss:  5679.8799 Validation Accuracy: 0.546875\n","Epoch  1, Batch  88 -Loss:  5657.4619 Validation Accuracy: 0.562500\n","Epoch  1, Batch  89 -Loss:  5461.3032 Validation Accuracy: 0.554688\n","Epoch  1, Batch  90 -Loss:  6252.5186 Validation Accuracy: 0.562500\n","Epoch  1, Batch  91 -Loss:  4991.0254 Validation Accuracy: 0.558594\n","Epoch  1, Batch  92 -Loss:  6379.8042 Validation Accuracy: 0.546875\n","Epoch  1, Batch  93 -Loss:  6291.7158 Validation Accuracy: 0.554688\n","Epoch  1, Batch  94 -Loss:  6717.0610 Validation Accuracy: 0.574219\n","Epoch  1, Batch  95 -Loss:  5513.4727 Validation Accuracy: 0.562500\n","Epoch  1, Batch  96 -Loss:  4899.3535 Validation Accuracy: 0.566406\n","Epoch  1, Batch  97 -Loss:  5470.7954 Validation Accuracy: 0.582031\n","Epoch  1, Batch  98 -Loss:  4802.6670 Validation Accuracy: 0.570312\n","Epoch  1, Batch  99 -Loss:  6299.9395 Validation Accuracy: 0.578125\n","Epoch  1, Batch 100 -Loss:  4829.2495 Validation Accuracy: 0.566406\n","Epoch  1, Batch 101 -Loss:  4672.7295 Validation Accuracy: 0.574219\n","Epoch  1, Batch 102 -Loss:  4275.7412 Validation Accuracy: 0.574219\n","Epoch  1, Batch 103 -Loss:  5218.5190 Validation Accuracy: 0.574219\n","Epoch  1, Batch 104 -Loss:  5431.5215 Validation Accuracy: 0.562500\n","Epoch  1, Batch 105 -Loss:  5469.7349 Validation Accuracy: 0.570312\n","Epoch  1, Batch 106 -Loss:  4690.2900 Validation Accuracy: 0.566406\n","Epoch  1, Batch 107 -Loss:  3157.2937 Validation Accuracy: 0.582031\n","Epoch  1, Batch 108 -Loss:  6625.4419 Validation Accuracy: 0.574219\n","Epoch  1, Batch 109 -Loss:  3570.7080 Validation Accuracy: 0.574219\n","Epoch  1, Batch 110 -Loss:  4590.0142 Validation Accuracy: 0.585938\n","Epoch  1, Batch 111 -Loss:  4804.5020 Validation Accuracy: 0.585938\n","Epoch  1, Batch 112 -Loss:  5429.9756 Validation Accuracy: 0.593750\n","Epoch  1, Batch 113 -Loss:  4360.9365 Validation Accuracy: 0.582031\n","Epoch  1, Batch 114 -Loss:  3692.8914 Validation Accuracy: 0.589844\n","Epoch  1, Batch 115 -Loss:  3048.7280 Validation Accuracy: 0.589844\n","Epoch  1, Batch 116 -Loss:  4698.2158 Validation Accuracy: 0.597656\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-32a033164bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 keep_prob: dropout})\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"z-fGKG6veBh-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}