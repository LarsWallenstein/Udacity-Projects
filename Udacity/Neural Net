{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Net","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"zZF_UX6GtCu8","colab_type":"code","colab":{}},"cell_type":"code","source":["#\n","#   Python Neural Networks code originally by Szabo Roland and used by permission\n","#\n","#   Modifications, comments, and exercise breakdowns by Mitchell Owen, (c) Udacity\n","#\n","#   Retrieved originally from http://rolisz.ro/2013/04/18/neural-networks-in-python/\n","#\n","#\n","#\tNeural Network Sandbox\n","#\n","#\tDefine an activation function activate(), which takes in a number and returns a number.\n","#\tUsing test run you can see the performance of a neural network running with that activation function.\n","#\n","import numpy as np\n","\n","\n","def activate(strength):\n","    return np.power(strength,2)\n","    \n","def activation_derivative(activate, strength):\n","    #numerically approximate\n","    return (activate(strength+1e-5)-activate(strength-1e-5))/(2e-5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dYr_8Z24uLWU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1a5197f9-321a-4460-8fb3-5932061eced0","executionInfo":{"status":"ok","timestamp":1547298240811,"user_tz":-180,"elapsed":22281,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["#\n","#   As with the perceptron exercise, you will modify the\n","#   last functions of this sigmoid unit class\n","#\n","#   There are two functions for you to finish:\n","#   First, in activate(), write the sigmoid activation function\n","#\n","#   Second, in train(), write the gradient descent update rule\n","#\n","#   NOTE: the following exercises creating classes for functioning\n","#   neural networks are HARD, and are not efficient implementations.\n","#   Consider them an extra challenge, not a requirement!\n","\n","import numpy as np\n","\n","def logD(x):\n","      return (1/(1+np.exp(-x))*(1-1/(1+np.exp(-x))))\n","  \n","class Sigmoid:\n","    \n","    weights = [1]\n","    last_input = 0\n","    \n","    def activate(self,values):\n","        '''Takes in @param values, @param weights lists of numbers\n","        and @param threshold a single number.\n","        @return the output of a threshold perceptron with\n","        given weights and threshold, given values as inputs.\n","        ''' \n","               \n","        #First calculate the strength with which the perceptron fires\n","        strength = self.strength(values)\n","        self.last_input = strength\n","        \n","        #YOUR CODE HERE\n","        #modify strength using the sigmoid activation function\n","        result = 1/(1+np.exp(-strength))\n","        \n","        return result\n","        \n","    def strength(self,values):\n","        strength = np.dot(values,self.weights)\n","        return strength\n","    \n","    \n","        \n","    def update(self,values,train,eta=.1):\n","        '''\n","        Updates the sigmoid unit with expected return\n","        values @param train and learning rate @param eta\n","        \n","        By modifying the weights according to the gradient descent rule\n","        '''\n","        \n","        #YOUR CODE HERE\n","        #modify the perceptron training rule to a gradient descent\n","        #training rule you will need to use the derivative of the\n","        #logistic function evaluated at the last input value.\n","        #Recall: d/dx logistic(x) = logistic(x)*(1-logistic(x))\n","        \n","        result = self.activate(values)\n","        for i in range(0,len(values)):\n","            self.weights[i] -= eta*(train-result)*(-result)*(1-result)*values[i]#Вот такая производная\n","        \n","    def __init__(self,weights=None):\n","        if weights:\n","            self.weights = weights\n","            \n","            \n","unit = Sigmoid(weights=[3,-2,1])\n","unit.update([1,2,3],[0])\n","print unit.weights\n","#Expected: [2.99075, -2.0185, .97225]"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[array([2.9907522]), array([-2.01849561]), array([0.97225659])]\n"],"name":"stdout"}]},{"metadata":{"id":"W1yGLzKj_INK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"fe4dec5b-3c96-4a9e-ece3-6736d696c6e6","executionInfo":{"status":"ok","timestamp":1547298364419,"user_tz":-180,"elapsed":234,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["unit.activate([1,2,3])"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.86651968])"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"d_QQ0gK2J14J","colab_type":"code","colab":{}},"cell_type":"code","source":["#\n","#   In the following exercises we will complete several functions for a \n","#   simple implementation of neural networks based on code by Roland \n","#   Szabo.\n","#   \n","#   In this exercise, we will will write a function, predict(),\n","#   which will predict the value of given inputs based on a constructed \n","#   network.\n","#\n","#   Note that we are not using the Sigmoid class we implemented earlier\n","#   to be able to compute more efficiently.\n","#\n","#   NOTE: the following exercises creating classes for functioning\n","#   neural networks are HARD, and are not efficient implementations.\n","#   Consider them an extra challenge, not a requirement!\n","\n","\n","\n","import numpy as np\n","\n","#choose a seed for testing in the exercise\n","np.random.seed(32)\n","\n","def logistic(x):\n","    return 1/(1 + np.exp(-x))\n","\n","def logistic_derivative(x):\n","    return logistic(x)*(1-logistic(x))\n","    \n","class NeuralNetwork:\n","\n","    def __init__(self, layers):\n","        \"\"\"\n","        :param layers: A list containing the number of units in each\n","          layer. Should be at least two values\n","        \"\"\"\n","        self.activation = logistic\n","        self.activation_deriv = logistic_derivative\n","        self.layers = len(layers)\n","        self.weights = []\n","        #randomly initialize weights)\n","        for i in range(1, len(layers) - 1):\n","            self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i]))-1)*0.25)\n","        i=len(layers)-2\n","        self.weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25)\n","\n","    def predict(self, x):\n","        \"\"\"\n","        :param x: a 1D ndarray of input values\n","        :return: a 1D ndarray of values of output nodes\n","        \"\"\"\n","        \n","        #YOUR CODE HERE\n","        for i in range(len(self.weights)):\n","          x=np.dot(self.weights[i][0:len(self.weights[i])-1].reshape(len(self.weights[i][0]),len(self.weights[i])-1),x)\n","          for j in range(len(x)):\n","            if(x[j]>self.weights[i][len(self.weights[i])-1][j]):\n","              x[j]=logistic(x[j])\n","            else:\n","              x[j]=0\n","        return x\n","        #our neural network is a numpy array self.weights\n","        #its first dimension is layers; self.weights[0] is the first\n","        #(input) layer.\n","        #its second dimension is nodes; self.weights[1][3] is the 4th \n","        #node in the second (hidden) layer.\n","        #its third dimension is weights; self.weights[1][3][2] will be \n","        #the weight assigned to the input from the third node on the \n","        #first layer.\n","        \n","        #for each layer, evaluate the nodes in that layer\n","        #by taking the dot product of the output of the previous layer\n","        #(or the input in the case of the first layer)\n","        #with the weights for that node, then applying the activation\n","        #function, self.activation()\n","        \n","        #also make sure to add a constant dummy value to the input by \n","        #appending 1 to it\n","        \n","        #return the output vector from the last layer.\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hscptI-tpQBw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4e40ebb5-f5c5-42c6-e42b-5104394a5a2b","executionInfo":{"status":"ok","timestamp":1547309476222,"user_tz":-180,"elapsed":563,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["n=NeuralNetwork([5,7,20,4,2,1])\n","n.predict([1,5, 67, 23, 1])"],"execution_count":127,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.52289775])"]},"metadata":{"tags":[]},"execution_count":127}]},{"metadata":{"id":"7ME45aCVxUhX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b5bb7ed9-1334-4199-9ccc-2bd80c8bf617","executionInfo":{"status":"ok","timestamp":1547308505616,"user_tz":-180,"elapsed":665,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["logistic(0.22782827*1+0.1183348*5)"],"execution_count":125,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.694130675630764"]},"metadata":{"tags":[]},"execution_count":125}]},{"metadata":{"id":"MZHhH9uFpyKM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"af9bcdf9-1e42-4761-eef6-ffc245be3c3e","executionInfo":{"status":"ok","timestamp":1547307680493,"user_tz":-180,"elapsed":755,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["n2=NeuralNetwork([3,4,2])\n","for i in range(len(n2.weights)):\n","  print n2.weights[i]\n","  print ' '"],"execution_count":107,"outputs":[{"output_type":"stream","text":["[[ 0.13820473 -0.10608523 -0.14484273  0.00205676]\n"," [-0.18343743 -0.16431807 -0.12726789 -0.22866823]\n"," [-0.13680657  0.06286606  0.06716152  0.01646984]\n"," [ 0.11769277  0.15758279 -0.06257191  0.15246125]]\n"," \n","[[ 0.1898247   0.04344462]\n"," [ 0.07792368 -0.10478188]\n"," [-0.0558164  -0.18634726]\n"," [-0.14359425  0.12752684]\n"," [ 0.03943534  0.06591585]]\n"," \n"],"name":"stdout"}]},{"metadata":{"id":"MlClsZBiQXZP","colab_type":"code","colab":{}},"cell_type":"code","source":["layers=[3,4,2]\n","weights = []\n","for i in range(1, len(layers) - 1):\n","  weights.append((2*np.random.random((layers[i]+1, layers[i-1]))-1)*0.25)\n","weights.append((2*np.random.random((layers[i]+1, layers[i -1]))-1)*0.25)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZdSVE2ygQYUZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"2236668d-bff4-4792-9310-3b988a6ee911","executionInfo":{"status":"ok","timestamp":1547307213270,"user_tz":-180,"elapsed":552,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["for i in range(len(weights)):\n","  print weights[i]\n","  print ' '"],"execution_count":100,"outputs":[{"output_type":"stream","text":["[[ 0.0264413  -0.03440719  0.14188081]\n"," [ 0.1003866   0.13820473 -0.10608523]\n"," [-0.14484273  0.00205676 -0.18343743]\n"," [-0.16431807 -0.12726789 -0.22866823]\n"," [-0.13680657  0.06286606  0.06716152]]\n"," \n","[[ 0.01646984  0.11769277  0.15758279]\n"," [-0.06257191  0.15246125  0.1898247 ]\n"," [ 0.04344462  0.07792368 -0.10478188]\n"," [-0.0558164  -0.18634726 -0.14359425]\n"," [ 0.12752684  0.03943534  0.06591585]]\n"," \n"],"name":"stdout"}]},{"metadata":{"id":"1pm3FRCxQz2x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"c868ab2c-578c-4539-a4c0-fc803041cdac","executionInfo":{"status":"ok","timestamp":1547304702411,"user_tz":-180,"elapsed":651,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["layers=[3,2,1]\n","weights = []\n","for i in range(1, len(layers) - 1):\n","  weights.append((2*np.random.random((layers[i - 1] + 1, layers[i]))-1)*0.25)\n","weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25)\n","for iint in range(len(weights)):\n","  print weights[iint]"],"execution_count":59,"outputs":[{"output_type":"stream","text":["[[ 0.05255843  0.07271427]\n"," [-0.15399588  0.1980687 ]\n"," [ 0.17530147  0.12316699]\n"," [-0.15986304 -0.15863266]]\n","[[0.07091349]\n"," [0.0946542 ]\n"," [0.00735268]]\n"],"name":"stdout"}]},{"metadata":{"id":"V8xyXXNJexJQ","colab_type":"code","colab":{}},"cell_type":"code","source":["x=np.dot(n2.weights[0][0:len(n2.weights[0])-1].reshape(len(n2.weights[0][0]),len(n2.weights[0])-1),[1,2,3])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ArEEEas7gqPh","colab_type":"code","colab":{}},"cell_type":"code","source":["x=np.dot(n2.weights[1][0:len(n2.weights[1])-1].reshape(len(n2.weights[1][0]),len(n2.weights[1])-1),x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R7g7r04Wg1Vw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"fdd77b70-0961-4c4d-c732-7ea3d8df98ea","executionInfo":{"status":"ok","timestamp":1547307959511,"user_tz":-180,"elapsed":793,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["x=np.dot(n2.weights[2][0:len(n2.weights[1])-1].reshape(len(n2.weights[1][0]),len(n2.weights[1])-1),x)"],"execution_count":111,"outputs":[{"output_type":"stream","text":["[-0.2371653   0.36255349]\n"],"name":"stdout"}]},{"metadata":{"id":"dhPslaeWlejY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"825fc37e-0b69-4d6b-982d-c3424f1552b9","executionInfo":{"status":"ok","timestamp":1547305353103,"user_tz":-180,"elapsed":712,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":[""],"execution_count":62,"outputs":[{"output_type":"stream","text":["[ 6 12]\n"],"name":"stdout"}]},{"metadata":{"id":"3sejVE5plfxA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"18eb1628-3ff0-41c2-a8a2-cac3f33d14e5","executionInfo":{"status":"ok","timestamp":1547308350862,"user_tz":-180,"elapsed":649,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["for i in range(3):\n","  print i\n","print i"],"execution_count":118,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","2\n"],"name":"stdout"}]},{"metadata":{"id":"LDB9Suffw7po","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9fb63d35-aa42-4d87-e2e4-0443fc0f77f7","executionInfo":{"status":"ok","timestamp":1547308357588,"user_tz":-180,"elapsed":494,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"cell_type":"code","source":["print i"],"execution_count":119,"outputs":[{"output_type":"stream","text":["2\n"],"name":"stdout"}]},{"metadata":{"id":"klie0Xglw9VI","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}